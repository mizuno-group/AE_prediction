{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Evaluation\n",
    "* It may work bacause this notebook dose not use the datasets whtch are not allowed to be uploaded\n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# machine learning libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# tabnet\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Methods\n",
    "def tbn_pred(x_train:pd.core.frame.DataFrame, y_train:pd.core.frame.DataFrame, x_val:pd.core.frame.DataFrame, y_val:pd.core.frame.DataFrame, x_test:pd.core.frame.DataFrame,params:dict, seed:int=24771):\n",
    "    batch_size=128\n",
    "    max_epochs=500\n",
    "    params['seed']=seed\n",
    "    pretrainer = TabNetPretrainer(**params)\n",
    "    pretrainer.fit(X_train=x_train.values,eval_set=[x_train.values],max_epochs=max_epochs,\n",
    "                patience=10,batch_size=batch_size,virtual_batch_size=128,\n",
    "                drop_last=True)\n",
    "    model = TabNetClassifier(**params)\n",
    "    model.fit(X_train=x_train.values,y_train=y_train.values,eval_set=[(x_val.values,y_val.values)],eval_name=[\"valid\"],\n",
    "            eval_metric=[\"logloss\"],max_epochs=max_epochs,patience=10,\n",
    "            batch_size=batch_size,virtual_batch_size=128,\n",
    "            drop_last=False, from_unsupervised=pretrainer)\n",
    "    oof_pred = model.predict_proba(x_val.values)[:,1]\n",
    "    y_pred = model.predict_proba(x_test.values)[:,1]\n",
    "    return oof_pred, y_pred\n",
    "\n",
    "def xgb_pred(x_train:pd.core.frame.DataFrame, y_train:pd.core.frame.DataFrame, x_val:pd.core.frame.DataFrame, y_val:pd.core.frame.DataFrame, x_test:pd.core.frame.DataFrame, params:dict, seed:int=24771):\n",
    "    params['seed']=seed\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dval = xgb.DMatrix(x_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(x_test)\n",
    "    evals = [(dtrain, 'train'), (dval, 'eval')]\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        evals = evals,\n",
    "        early_stopping_rounds=20,\n",
    "        num_boost_round=10000,\n",
    "        verbose_eval=0,\n",
    "        )\n",
    "    oof_pred = model.predict(dval)\n",
    "    y_pred = model.predict(dtest)\n",
    "    return oof_pred, y_pred\n",
    "\n",
    "def lgb_pred(x_train:pd.core.frame.DataFrame, y_train:pd.core.frame.DataFrame, x_val:pd.core.frame.DataFrame, y_val:pd.core.frame.DataFrame, x_test:pd.core.frame.DataFrame, params:dict, seed:int=24771):\n",
    "    params['seed']=seed\n",
    "    train_dataset = lgb.Dataset(x_train, y_train, params={'verbose': -1})\n",
    "    val_dataset = lgb.Dataset(x_val, y_val, params={'verbose': -1})\n",
    "    model = lgb.train(params = params, \n",
    "                        train_set = train_dataset, \n",
    "                        valid_sets = [train_dataset, val_dataset], \n",
    "                        num_boost_round = 10000, \n",
    "                        early_stopping_rounds = 20, \n",
    "                        verbose_eval = False,\n",
    "                        )\n",
    "    oof_pred = model.predict(x_val)\n",
    "    y_pred = model.predict(x_test)\n",
    "    return oof_pred, y_pred\n",
    "\n",
    "def nb_pred(x_train:pd.core.frame.DataFrame, y_train:pd.core.frame.DataFrame, x_val:pd.core.frame.DataFrame, y_val:pd.core.frame.DataFrame, x_test:pd.core.frame.DataFrame, params:dict, seed:int=24771):\n",
    "    model = GaussianNB()\n",
    "    model.fit(x_train,y_train)\n",
    "    oof_pred = model.predict_proba(x_val)[:,1]\n",
    "    y_pred = model.predict_proba(x_test)[:,1]\n",
    "    return oof_pred, y_pred\n",
    "\n",
    "def svm_pred(x_train:pd.core.frame.DataFrame, y_train:pd.core.frame.DataFrame, x_val:pd.core.frame.DataFrame, y_val:pd.core.frame.DataFrame, x_test:pd.core.frame.DataFrame, params:dict, seed:int=24771):\n",
    "    params['random_state']=seed\n",
    "    model = SVC(**params, probability=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    oof_pred = model.predict_proba(x_val)[:,1]\n",
    "    y_pred = model.predict_proba(x_test)[:,1]\n",
    "    return oof_pred, y_pred\n",
    "\n",
    "def els_pred(x_train:pd.core.frame.DataFrame, y_train:pd.core.frame.DataFrame, x_val:pd.core.frame.DataFrame, y_val:pd.core.frame.DataFrame, x_test:pd.core.frame.DataFrame, params:dict, seed:int=24771):\n",
    "    params[\"random_state\"]=seed\n",
    "    model = ElasticNet(**params)\n",
    "    model.fit(x_train, y_train)\n",
    "    oof_pred = model.predict(x_val)\n",
    "    y_pred = model.predict(x_test)\n",
    "    return oof_pred, y_pred\n",
    "\n",
    "# Parameters \n",
    "lgb_params=  {\n",
    "    \"num_leaves\": 2**5,\n",
    "    \"min_data_in_leaf\": 5,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"feature_fraction\": 0.7,\n",
    "    \"lambda_l1\": 1e-5,\n",
    "    \"lambda_l2\": 1e-5,\n",
    "    'boosting': 'gbdt',\n",
    "    \"objective\":\"binary\",\n",
    "    \"metric\":\"binary_logloss\",\n",
    "    \"learning_rate\":0.005,\n",
    "    'seed':24771,\n",
    "    'verbose':-1\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 1e-5,\n",
    "    'max_delta_step': 5,\n",
    "    'lambda': 1e-5,\n",
    "    'alpha': 1e-5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.03,\n",
    "    'seed': 24771,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': 0.05,\n",
    "    'kernel':'sigmoid',\n",
    "    'degree': 5,\n",
    "    'gamma': 'auto',\n",
    "    'coef0': 0.4\n",
    "}\n",
    "\n",
    "tbn_params = {\n",
    "    'n_d':60,\n",
    "    'n_a':60,\n",
    "    'n_independent':2,\n",
    "    'n_shared':2,\n",
    "    'mask_type': 'sparsemax',\n",
    "    'n_steps': 1,\n",
    "    'gamma': 1.2,\n",
    "    'n_shared': 2,\n",
    "    'lambda_sparse': 1e-05,\n",
    "    'optimizer_fn':torch.optim.Adam, \n",
    "    'optimizer_params':dict(lr=2e-2),\n",
    "    'scheduler_params':dict(mode=\"min\",\n",
    "                            patience=5,\n",
    "                            min_lr=1e-7,\n",
    "                            factor=0.9),\n",
    "    'scheduler_fn':torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'verbose':0,\n",
    "    'device_name':\"cpu\"\n",
    "}\n",
    "\n",
    "els_params= {\n",
    "    'alpha':0.01,\n",
    "    'l1_ratio':0.2,\n",
    "    'max_iter':10000,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "def preprocess(x_train:pd.core.frame.DataFrame, x_test:pd.core.frame.DataFrame):\n",
    "    # standalize / input nan\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(x_train)\n",
    "    x_train = pd.DataFrame(ss.transform(x_train), index=x_train.index, columns=x_train.columns).fillna(0)\n",
    "    x_test = pd.DataFrame(ss.transform(x_test), index=x_test.index, columns=x_test.columns).fillna(0)\n",
    "    return x_train, x_test\n",
    "\n",
    "def create_model(x:pd.core.frame.DataFrame, x_test:pd.core.frame.DataFrame, y:pd.core.frame.DataFrame, params:dict, method:Callable=\"\", seed:int=0):\n",
    "    # Create a KFold object\n",
    "    fold = StratifiedKFold(n_splits = 5, random_state = seed, shuffle = True)\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "\n",
    "    # Iterate through each fold\n",
    "    predictions = np.zeros(x_test.shape[0])\n",
    "    for trn_ind, val_ind in fold.split(x, y):\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        oof_pred, y_pred = method(x_train, y_train, x_val, y_val, x_test, params, seed=seed)\n",
    "        predictions += y_pred / 5\n",
    "        oof_predictions[val_ind] = oof_pred\n",
    "        gc.collect()\n",
    "    return oof_predictions, predictions\n",
    "\n",
    "def calc_statistics(y_true:np.ndarray, y_pred:np.ndarray, threshold:float):\n",
    "    # For minus-AUPRC\n",
    "    rev_y_true = [1-i for i in y_true]\n",
    "    rev_y_pred = [1-i for i in y_pred]\n",
    "    # calculation\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "    auroc = metrics.auc(fpr, tpr)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_pred)\n",
    "    auprc = metrics.auc(recall, precision)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(rev_y_true, rev_y_pred)\n",
    "    minusauprc = metrics.auc(recall, precision)\n",
    "    # other scores\n",
    "    y_dammy = [1 if i>threshold else 0 for i in y_pred]\n",
    "    acc = metrics.accuracy_score(y_true, y_dammy)\n",
    "    f1 = metrics.matthews_corrcoef(y_true, y_dammy)\n",
    "    mcc = metrics.f1_score(y_true=y_true, y_pred=y_dammy)\n",
    "    return auroc, auprc, minusauprc, acc, f1, mcc\n",
    "\n",
    "def information(target:str=\"\", file_split:str=\"\", file_X:str=\"\", file_y:str=\"\", file_filtered_feature:str=\"\"):\n",
    "    x, x_test, y, y_test = load(target, file_split=file_split, file_X=file_X, file_y=file_y, file_filtered_feature=file_filtered_feature, fillna=False)\n",
    "    train_all = len(x.index)\n",
    "    train_p = sum(y)\n",
    "    train_n = train_all - train_p\n",
    "    test_all = len(x_test.index)\n",
    "    test_p = sum(y_test)\n",
    "    test_n = test_all - test_p\n",
    "    all_compounds = len(x.index)+len(x_test.index)\n",
    "    return (all_compounds, train_all, train_p, train_n, test_all, test_p, test_n)\n",
    "\n",
    "def ensemble_mean(lst_oof_preds, lst_preds, y_train, **kwargs):\n",
    "    return pd.DataFrame(lst_oof_preds).mean().values, pd.DataFrame(lst_preds).mean().values\n",
    "\n",
    "def pred(method:Callable=\"\", params:dict=dict(), random_state:int=0, target:str=\"\", file_split:str=\"\", file_X:str=\"\", file_y:str=\"\", file_filtered_feature:str=\"\", fillna:bool=False, features:list=[], ensemble:Callable=\"\"):\n",
    "    x, x_test, y, y_test = load(target, file_split=file_split, file_X=file_X, file_y=file_y, file_filtered_feature=file_filtered_feature, fillna=fillna)\n",
    "    oof_predictions = []\n",
    "    predictions = []\n",
    "    seed = random_state\n",
    "    for v in range(5):\n",
    "        lst_oof_preds = []\n",
    "        lst_preds = []\n",
    "        for feature in features:\n",
    "            x_temp, x_test_temp = x.loc[:,feature], x_test.loc[:,feature]\n",
    "            oof_preds, preds = create_model(x_temp, x_test_temp, y, params, method=method, seed=seed)\n",
    "            lst_oof_preds.append(oof_preds)\n",
    "            lst_preds.append(preds)\n",
    "        oof_preds, preds = ensemble(lst_oof_preds, lst_preds, y)\n",
    "        oof_predictions.append(oof_preds)\n",
    "        predictions.append(preds)\n",
    "        seed += 1\n",
    "    return oof_predictions, predictions\n",
    "\n",
    "def load(target:str, file_split:str=\"\", file_X:str=\"\", file_y:str=\"\", file_filtered_feature:str=\"\", fillna:bool=True):\n",
    "    X = pd.read_pickle(file_X)\n",
    "    y = pd.read_pickle(file_y)\n",
    "    train_comp, test_comp = pd.read_pickle(file_split)\n",
    "    filtered_feature = pd.read_pickle(file_filtered_feature)\n",
    "    X_train = X.loc[train_comp, filtered_feature]\n",
    "    X_test = X.loc[test_comp, filtered_feature]\n",
    "    y_train = y.loc[train_comp, target]\n",
    "    y_test = y.loc[test_comp, target]\n",
    "    if fillna:\n",
    "        X_train, X_test = preprocess(X_train, X_test)\n",
    "    X_test.columns = X_train.columns.tolist()\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main modules\n",
    "def main_date(method_def:Callable, params:dict=dict(), method:str=\"\", folder:str=\"\", fillna:bool=False, ensemble:Callable=\"\"):\n",
    "    # path / cols for result\n",
    "    file_X = \"data/X.pickle\"\n",
    "    file_y = \"data/y.pickle\"\n",
    "    target = 'Hepatobiliary disorders'\n",
    "    col = [\n",
    "        \"all_compounds\",\n",
    "        \"train_all\",\n",
    "        \"train_positive\",\n",
    "        \"train_negative\",\n",
    "        \"test_all\",\n",
    "        \"test_positive\",\n",
    "        \"test_negative\",\n",
    "        \"feature_number\",\n",
    "        \"model\",\n",
    "        \"threhold\",\n",
    "        \"auroc_oof\",\n",
    "        \"aupr_oof\",\n",
    "        \"minusauprc_oof\",\n",
    "        \"accuracy_oof\",\n",
    "        \"f1score_oof\",\n",
    "        \"mcc_oof\",\n",
    "        \"auroc_test\",\n",
    "        \"aupr_test\",\n",
    "        \"minusauprc_test\",\n",
    "        \"accuracy_test\",\n",
    "        \"f1score_test\",\n",
    "        \"mcc_test\",\n",
    "    ]\n",
    "    names=[\n",
    "        \"mold2\", \"mol2vec\", \"mordred\", \"pubchem\", \"admet\",\n",
    "    ]\n",
    "    random_state=24771\n",
    "    df_res = pd.DataFrame(columns=col)\n",
    "    for i in tqdm(range(20)):\n",
    "        output = f\"ensemble_models/date/{method}_{str(i)}.pickle\"\n",
    "        file_filtered_feature = \"data/all_features.pickle\"\n",
    "        file_split = \"data/comp_split/date.pickle\" # date split file path\n",
    "        informations = information(target, file_split=file_split, file_X=file_X, file_y=file_y, file_filtered_feature=file_filtered_feature)\n",
    "        feature_numbers = 0\n",
    "        features = [pd.read_pickle(f\"data/filtered_feature/{v}/date.pickle\") for v in names]\n",
    "        preds = pred(\n",
    "            method=method_def, params=params, random_state=random_state, \n",
    "            target=target, file_split=file_split, file_X=file_X, file_y=file_y,file_filtered_feature=file_filtered_feature,\n",
    "            fillna=fillna, features=features, ensemble=ensemble, output=output)\n",
    "        x, x_test, y, y_test = load(target, file_split=file_split, file_X=file_X, file_y=file_y, file_filtered_feature=file_filtered_feature, fillna=False)\n",
    "        # threshold\n",
    "        thresh=0.5\n",
    "        # calc scores\n",
    "        oof_preds = pd.DataFrame(preds[0]).mean().values\n",
    "        res_oof = calc_statistics(y, oof_preds, thresh)\n",
    "        y_preds = pd.DataFrame(preds[1]).mean().values\n",
    "        res_test = calc_statistics(y_test, y_preds, thresh)\n",
    "        res_all = [*informations, feature_numbers, method, thresh, *res_oof, *res_test]\n",
    "        df_res.loc[str(i),:] = res_all\n",
    "        random_state+=10\n",
    "    # output\n",
    "    df_res.to_pickle(\"evaluation/date/\"+method+\"_\"+folder+\".pickle\")\n",
    "    print(f\"finished : {method} / {folder}\")\n",
    "\n",
    "def main_random(method_def:Callable, params:dict=dict(), method:str=\"\", folder:str=\"\", fillna:bool=False, ensemble:Callable=\"\"):\n",
    "    # path / cols for result\n",
    "    file_X = \"data/X.pickle\"\n",
    "    file_y = \"data/y.pickle\"\n",
    "    target = 'Hepatobiliary disorders'\n",
    "    col = [\n",
    "        \"all_compounds\",\n",
    "        \"train_all\",\n",
    "        \"train_positive\",\n",
    "        \"train_negative\",\n",
    "        \"test_all\",\n",
    "        \"test_positive\",\n",
    "        \"test_negative\",\n",
    "        \"feature_number\",\n",
    "        \"model\",\n",
    "        \"threhold\",\n",
    "        \"auroc_oof\",\n",
    "        \"aupr_oof\",\n",
    "        \"minusauprc_oof\",\n",
    "        \"accuracy_oof\",\n",
    "        \"f1score_oof\",\n",
    "        \"mcc_oof\",\n",
    "        \"auroc_test\",\n",
    "        \"aupr_test\",\n",
    "        \"minusauprc_test\",\n",
    "        \"accuracy_test\",\n",
    "        \"f1score_test\",\n",
    "        \"mcc_test\",\n",
    "    ]\n",
    "    names=[\n",
    "        \"mold2\", \"mol2vec\", \"mordred\", \"pubchem\", \"admet\",\n",
    "    ]\n",
    "    random_state=24771\n",
    "    df_res = pd.DataFrame(columns=col)\n",
    "    for i in range(20):\n",
    "        file_filtered_feature = \"data/all_features.pickle\"\n",
    "        file_split = f\"data/comp_split/random_{str(i)}.pickle\" # date split file path\n",
    "        informations = information(target, file_split=file_split, file_X=file_X, file_y=file_y, file_filtered_feature=file_filtered_feature)\n",
    "        feature_numbers = 0\n",
    "        features = [pd.read_pickle(f\"data/filtered_feature/{v}/random_{str(i)}.pickle\") for v in names]\n",
    "        preds = pred(\n",
    "            method=method_def, params=params, random_state=random_state, \n",
    "            target=target, file_split=file_split, file_X=file_X, file_y=file_y,file_filtered_feature=file_filtered_feature,\n",
    "            fillna=fillna, features=features, ensemble=ensemble)\n",
    "        x, x_test, y, y_test = load(target, file_split=file_split, file_X=file_X, file_y=file_y, file_filtered_feature=file_filtered_feature, fillna=False)\n",
    "        # threshold\n",
    "        thresh=0.5\n",
    "        # calc scores\n",
    "        oof_preds = pd.DataFrame(preds[0]).mean().values\n",
    "        res_oof = calc_statistics(y, oof_preds, thresh)\n",
    "        y_preds = pd.DataFrame(preds[1]).mean().values\n",
    "        res_test = calc_statistics(y_test, y_preds, thresh)\n",
    "        res_all = [*informations, feature_numbers, method, thresh, *res_oof, *res_test]\n",
    "        df_res.loc[str(i),:] = res_all\n",
    "        random_state+=10\n",
    "\n",
    "    # output\n",
    "    df_res.to_pickle(\"evaluation/random/\"+method+\"_\"+folder+\".pickle\")\n",
    "    print(f\"finished : {method} / {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"ensemble_mean_bind\"\n",
    "pred_lst = [\n",
    "    [xgb_pred, xgb_params, \"xgb\"],\n",
    "    [lgb_pred, lgb_params, \"lgb\"],\n",
    "]\n",
    "for i in pred_lst:\n",
    "    main_date(i[0], params=i[1], method=i[2], folder=folder, fillna=False, ensemble=ensemble_mean)\n",
    "    main_random(i[0], params=i[1], method=i[2], folder=folder, fillna=False, ensemble=ensemble_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lst = [\n",
    "    [els_pred, els_params, \"els\"],\n",
    "    [nb_pred, {}, \"nb\"],\n",
    "    [svm_pred, svm_params, \"svm\"],\n",
    "    [tbn_pred, tbn_params, \"tbn\"],\n",
    "]\n",
    "for i in pred_lst:\n",
    "    main_date(i[0], params=i[1], method=i[2], folder=folder, fillna=True, ensemble=ensemble_mean)\n",
    "    main_random(i[0], params=i[1], method=i[2], folder=folder, fillna=True, ensemble=ensemble_mean)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47b53935d5cbd3efb4eccc036f7c25f7b7fe7cf3075707571baeab5fe050364f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('faers': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
