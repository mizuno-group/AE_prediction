{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compounds split with \"Date\" or \"Random\"\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some datasets are not allowed to be uploaded.\n",
    "* Therefore, some datasets have been removed from publication environment.\n",
    "* The locations of the non-working without these datasets are commented out.\n",
    "* The output that differ from the publication environment are also commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standalize_df(df:pd.core.frame.DataFrame):\n",
    "    \"\"\" return z score \"\"\"\n",
    "    return (df - df.mean()) / df.std(ddof=0)\n",
    "\n",
    "def index_intersection(df:pd.core.frame.DataFrame,df2:pd.core.frame.DataFrame):\n",
    "    \"\"\" return same index dataframe (lower) \"\"\"\n",
    "    ind1 = list(df.index)\n",
    "    ind2 = list(df2.index)\n",
    "    ind1 = [i.lower() for i in ind1]\n",
    "    ind2 = [i.lower() for i in ind2]\n",
    "    df.index=ind1\n",
    "    df2.index=ind2\n",
    "    ind_new = set(ind1) & set(ind2)\n",
    "    ind_new = list(ind_new)\n",
    "    df = copy.deepcopy(df.loc[ind_new,:])\n",
    "    df2 = copy.deepcopy(df2.loc[ind_new,:])\n",
    "    return df, df2\n",
    "\n",
    "def load_X(comp_thresh:float=0, feature_thresh:float=0, join:str='inner'):\n",
    "    # load all data and concat\n",
    "    names = [\"drugbank\", \"ctd\", \"semmed\", \"l1000\", \"mold2\", \"mol2vec\", \"mordred\", \"pubchem\", \"admet\"] # not-working \n",
    "    files = [f\"dataset/{i}.csv\" for i in names]\n",
    "    \n",
    "    for i, (name, file) in enumerate(zip(names, files)):\n",
    "        if i==0:\n",
    "            df_mol = pd.read_csv(file, index_col=0)\n",
    "            df_mol = nan_threshold(df_mol, threshold=comp_thresh,axis=0)\n",
    "            df_mol = nan_threshold(df_mol, threshold=feature_thresh,axis=1)\n",
    "            df_mol.columns=[f\"{name}_{str(i)}\" for i in df_mol.columns]\n",
    "            print(f\"{name} : {df_mol.shape}\")\n",
    "        else:\n",
    "            df_temp = pd.read_csv(file, index_col=0)\n",
    "            df_temp = nan_threshold(df_temp, threshold=comp_thresh,axis=0)\n",
    "            df_temp = nan_threshold(df_temp, threshold=feature_thresh,axis=1)\n",
    "            print(f\"{name} : {df_temp.shape}\")\n",
    "            df_temp.columns=[f\"{name}_{str(i)}\" for i in df_temp.columns]\n",
    "            df_mol = pd.concat([df_mol,df_temp],axis=1,sort=False, join=join)\n",
    "    df_mol = df_mol.dropna(how=\"all\",axis=1)\n",
    "    return df_mol\n",
    "\n",
    "def nan_threshold(df:pd.core.frame.DataFrame,threshold:float=0,axis:int=0):\n",
    "    \"\"\" remove missing features and compounds \"\"\"\n",
    "    if threshold==0:\n",
    "        df = df.dropna(how=\"any\",axis=axis)\n",
    "    else:\n",
    "        if axis==0:\n",
    "            df = df.T.loc[:,df.T.isnull().sum()<(len(df.columns)*threshold)]\n",
    "            df = df.T\n",
    "        elif axis==1:\n",
    "            df = df.loc[:,df.isnull().sum()<(len(df.index)*threshold)]\n",
    "    return df\n",
    "\n",
    "def load_df(comp_thresh:float=0, feature_thresh:float=0, join:str='inner', file_sider:str=\"dataset/sider.csv\"):\n",
    "    \"\"\" main module to load and concatenate all files \"\"\"\n",
    "    X = load_X(comp_thresh=comp_thresh, feature_thresh=feature_thresh, join=join)\n",
    "    y = pd.read_csv(file_sider, index_col=0)\n",
    "    X, y = index_intersection(X, y)\n",
    "    return X, y\n",
    "\n",
    "def date_split_number(df_dates:pd.core.frame.DataFrame, number:int=10):\n",
    "    \"\"\" date split from date dataframe \"\"\"\n",
    "    df_dates = df_dates.sort_values(by=['date'], ascending=False)\n",
    "    train_comp = df_dates['name'].tolist()[number:]\n",
    "    test_comp = df_dates['name'].tolist()[:number]\n",
    "    return train_comp, test_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugbank : (791, 882)\n",
      "ctd : (984, 21650)\n",
      "semmed : (1179, 5155)\n",
      "l1000 : (900, 958)\n",
      "mold2 : (791, 644)\n",
      "mol2vec : (1089, 300)\n",
      "mordred : (791, 1517)\n",
      "pubchem : (1087, 30)\n",
      "admet : (791, 106)\n",
      "all data : (451, 31242)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X, y=load_df(comp_thresh=1, feature_thresh=1, join='inner')\n",
    "print(f\"all data : {X.shape}\")\n",
    "compounds = X.index.tolist()\n",
    "\n",
    "# load date information\n",
    "df = pd.read_csv(\"dataset/drugbank_date_information.csv\", index_col=0)\n",
    "compounds_inter = set(df[\"name\"].tolist()) & set(compounds)\n",
    "df.index = df[\"name\"]\n",
    "df = df.loc[list(compounds_inter),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# date split\n",
    "train_comp, test_comp = date_split_number(df, number=int(.2*len(X.index)))\n",
    "print(len(train_comp))\n",
    "print(len(test_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc\n",
    "X = X.loc[compounds_inter,:]\n",
    "y = y.loc[compounds_inter,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all data for next step (output will changed without some datasets)\n",
    "pd.to_pickle(X, \"data/X.pickle\") # Changed\n",
    "pd.to_pickle(y, \"data/y.pickle\")\n",
    "pd.to_pickle(X.columns.tolist(), \"data/all_features.pickle\")\n",
    "pd.to_pickle(list(compounds_inter), \"data/all_comp.pickle\")\n",
    "pd.to_pickle([train_comp, test_comp], \"data/comp_split/date.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compound split by random\n",
    "seed = 24771\n",
    "compounds_all = pd.read_pickle(\"data/all_comp.pickle\")\n",
    "for i in range(20):\n",
    "    random.seed(seed)\n",
    "    test_comp = random.sample(compounds_all, 90)\n",
    "    train_comp = list(set(compounds_all) - set(test_comp))\n",
    "    pd.to_pickle([train_comp, test_comp], f\"data/comp_split/random_{str(i)}.pickle\")\n",
    "    seed += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering features\n",
    "* calculate and export filtering features of each data \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering modules\n",
    "def same_feature(df):\n",
    "    \"\"\" delete same feature \"\"\"\n",
    "    drop = df.loc[:,df.T.duplicated()].columns.tolist()\n",
    "    return df.drop(drop, axis=1)\n",
    "\n",
    "def cv_limit(df, threshold=0.05):\n",
    "    \"\"\" delete low CV feature \"\"\"\n",
    "    drop = df.loc[:,(df.std() / df.abs().mean())< threshold].columns\n",
    "    return df.drop(drop, axis=1)\n",
    "\n",
    "def corr(df):\n",
    "    \"\"\" calculate correlation \"\"\"\n",
    "    values = 1 - squareform(pdist(df.T, 'correlation'))\n",
    "    return pd.DataFrame(values,\n",
    "                        columns=df.columns, index=df.columns)\n",
    "\n",
    "def corr_limit(df, threshold=0.85):\n",
    "    \"\"\" delete high correlation feature \"\"\"\n",
    "    # calc correlation\n",
    "    corr_matrix = corr(df)\n",
    "\n",
    "    # drop phase\n",
    "    drop = []\n",
    "    flag=True\n",
    "    while flag:\n",
    "        try:\n",
    "            if corr_matrix.iloc[0,0]==1:\n",
    "                corr_matrix = corr_matrix.iloc[1:,:]\n",
    "        except:\n",
    "            break\n",
    "        temp_lst = corr_matrix[corr_matrix.iloc[:,0]>threshold].index.tolist()\n",
    "        if len(temp_lst)>0:\n",
    "            drop+=temp_lst\n",
    "            corr_matrix = corr_matrix.drop(temp_lst, axis=0).drop(temp_lst, axis=1)\n",
    "        else:\n",
    "            corr_matrix = corr_matrix.iloc[1:,1:]\n",
    "        if len(corr_matrix.columns)<2:\n",
    "            flag=False\n",
    "    del corr_matrix\n",
    "    return df.drop(drop, axis=1)\n",
    "\n",
    "def filter(df, cv:float=0.05, corr:float=0.85):\n",
    "    \"\"\" main feature filtering module \"\"\"\n",
    "    features = df.columns.tolist() \n",
    "    df = same_feature(df)\n",
    "    df = cv_limit(df, threshold=cv)\n",
    "    df = corr_limit(df, threshold=corr)\n",
    "    features_new = df.columns.tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_X = \"data/X.pickle\"\n",
    "names = [\"drugbank\", \"ctd\", \"semmed\", \"l1000\", \"mold2\", \"mol2vec\", \"mordred\", \"pubchem\", \"admet\"] # not-working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "X = pd.read_pickle(file_X)\n",
    "\n",
    "# date split\n",
    "train_comp, test_comp = pd.read_pickle(\"data/comp_split/date.pickle\")\n",
    "for name in names:\n",
    "    X_train = copy.deepcopy(X.loc[train_comp, X.columns.str.contains(name)])\n",
    "    X_train = filter(X_train, cv=0.05, corr=0.85)\n",
    "    filtered_feature = X_train.columns.tolist()\n",
    "    pd.to_pickle(filtered_feature, f\"data/filtered_feature/{name}/date.pickle\")\n",
    "\n",
    "# random split\n",
    "for i in range(20):\n",
    "    train_comp, test_comp = pd.read_pickle(f\"data/comp_split/random_{str(i)}.pickle\")\n",
    "    for name in names:\n",
    "        X_train = copy.deepcopy(X.loc[train_comp, X.columns.str.contains(name)])\n",
    "        X_train = filter(X_train, cv=0.05, corr=0.85)\n",
    "        filtered_feature = X_train.columns.tolist()\n",
    "        pd.to_pickle(filtered_feature, f\"data/filtered_feature/{name}/random_{str(i)}.pickle\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Concat dataset (without protein intaraction datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"concat_bind\"\n",
    "# load\n",
    "X = pd.read_pickle(file_X)\n",
    "\n",
    "# date\n",
    "train, test = pd.read_pickle(\"data/comp_split/date.pickle\")\n",
    "features = []\n",
    "for name in names:\n",
    "    features += pd.read_pickle(f\"data/filtered_feature/{name}/date.pickle\")\n",
    "X_train = copy.deepcopy(X.loc[train, features])\n",
    "X_train = filter(X_train, cv=0.05, corr=0.85)\n",
    "features = X_train.columns.tolist()\n",
    "pd.to_pickle(features, f\"data/filtered_feature/{folder}/date.pickle\")\n",
    "\n",
    "# random\n",
    "for i in range(20):\n",
    "    train, test = pd.read_pickle(f\"data/comp_split/random_{str(i)}.pickle\")\n",
    "    features = []\n",
    "    for name in names:\n",
    "        features += pd.read_pickle(f\"data/filtered_feature/{name}/random_{str(i)}.pickle\")\n",
    "    X_train = copy.deepcopy(X.loc[train, features])\n",
    "    X_train = filter(X_train, cv=0.05, corr=0.85)\n",
    "    features = X_train.columns.tolist()\n",
    "    pd.to_pickle(features, f\"data/filtered_feature/{folder}/random_{str(i)}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binding data inter section\n",
    "* For binding data set (semmed, ctd, drugbank), generate filtered features with common features\n",
    "* It will not work bacause 2/3 datasets are not allowed to uploaded.\n",
    "* Only filtered features of Drugbank dataset are uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank = X.loc[:,X.columns.str.contains(\"drugbank_\")].columns.tolist()\n",
    "ctd = X.loc[:,X.columns.str.contains(\"ctd_\")].columns.tolist()\n",
    "semmed = X.loc[:,X.columns.str.contains(\"semmed_\")].columns.tolist()\n",
    "\n",
    "drugbank = [i.split(\"drugbank_\")[1] for i in drugbank]\n",
    "ctd = [i.split(\"ctd_\")[1] for i in ctd]\n",
    "semmed = [i.split(\"semmed_\")[1] for i in semmed]\n",
    "\n",
    "feature_intersection = list(set(drugbank)&set(ctd)&set(semmed))\n",
    "drugbank_features = [f\"drugbank_{i}\" for i in feature_intersection]\n",
    "ctd_features = [f\"ctd_{i}\" for i in feature_intersection]\n",
    "semmed_features = [f\"semmed_{i}\" for i in feature_intersection]\n",
    "\n",
    "names = [\"drugbank_inter\", \"ctd_inter\", \"semmed_inter\"]\n",
    "features_lst = [drugbank_features, ctd_features, semmed_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "X = pd.read_pickle(file_X)\n",
    "\n",
    "# date split\n",
    "train_comp, test_comp = pd.read_pickle(\"data/comp_split/date.pickle\")\n",
    "for name, features in zip(names, features_lst):\n",
    "    X_train = copy.deepcopy(X.loc[train_comp, features])\n",
    "    X_train = filter(X_train, cv=0.05, corr=0.85)\n",
    "    filtered_feature = X_train.columns.tolist()\n",
    "    pd.to_pickle(filtered_feature, f\"data/filtered_feature/{name}/date.pickle\")\n",
    "\n",
    "# random split\n",
    "for i in range(20):\n",
    "    train_comp, test_comp = pd.read_pickle(f\"data/comp_split/random_{str(i)}.pickle\")\n",
    "    for name, features in zip(names, features_lst):\n",
    "        X_train = copy.deepcopy(X.loc[train_comp, features])\n",
    "        X_train = filter(X_train, cv=0.05, corr=0.85)\n",
    "        filtered_feature = X_train.columns.tolist()\n",
    "        pd.to_pickle(filtered_feature, f\"data/filtered_feature/{name}/random_{str(i)}.pickle\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47b53935d5cbd3efb4eccc036f7c25f7b7fe7cf3075707571baeab5fe050364f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('faers': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
